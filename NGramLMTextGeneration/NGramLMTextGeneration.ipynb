{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-28T21:08:40.695393Z",
     "start_time": "2025-02-28T21:08:23.356149Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import trigrams, bigrams\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download necessary NLTK datasets (if not already available)\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ade3a0459d20f682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:14:19.695848Z",
     "start_time": "2025-02-28T21:14:08.846379Z"
    }
   },
   "source": [
    "# Tokenize the Reuters corpus into words\n",
    "words = nltk.word_tokenize(' '.join(reuters.words()))\n",
    "\n",
    "# Create the trigram model\n",
    "tri_grams = list(trigrams(words))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1a4fed580ab3621c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:14:50.226300Z",
     "start_time": "2025-02-28T21:14:48.022138Z"
    }
   },
   "source": [
    "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for w1, w2, w3 in tri_grams:\n",
    "    trigram_model[(w1, w2)][w3] += 1"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "12ed5a26a492f15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:14:51.647777Z",
     "start_time": "2025-02-28T21:14:51.260238Z"
    }
   },
   "source": [
    "for w1_w2 in trigram_model:\n",
    "    total_count = float(sum(trigram_model[w1_w2].values()))\n",
    "    for w3 in trigram_model[w1_w2]:\n",
    "        trigram_model[w1_w2][w3] /= total_count"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ad31be27ebcb210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:14:56.447193Z",
     "start_time": "2025-02-28T21:14:56.443788Z"
    }
   },
   "source": [
    "def predict_next_word(trigram_model,w1, w2):\n",
    "    next_word = trigram_model[w1, w2]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "c501ed66ec99bdb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:15:02.842421Z",
     "start_time": "2025-02-28T21:15:02.838429Z"
    }
   },
   "source": [
    "print(\"Next Word:\", predict_next_word(trigram_model,'the', 'stock'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word: of\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "378ada94cd0d653c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:15:05.284365Z",
     "start_time": "2025-02-28T21:15:05.276853Z"
    }
   },
   "source": [
    "def generate_sentence_trigram(trigram_model, w1, w2, num_words=10):\n",
    "    sentence = [w1, w2]\n",
    "    for _ in range(num_words):\n",
    "        next_word = predict_next_word(trigram_model,w1, w2)\n",
    "        if not next_word:\n",
    "            break\n",
    "        w3 = next_word  # Choose the most probable next word\n",
    "        sentence.append(w3)\n",
    "        w1, w2 = w2, w3  # Shift words for the next trigram prediction\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Test the trigram-based sentence generation\n",
    "print(generate_sentence_trigram(trigram_model, 'the', 'stock'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stock of the company ' s & lt ; BP >\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c0f03f9fb2840a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:15:16.752684Z",
     "start_time": "2025-02-28T21:15:15.520639Z"
    }
   },
   "source": [
    "bi_grams = list(bigrams(words))\n",
    "\n",
    "# Initialize a defaultdict for the bigram model\n",
    "bigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Build the bigram model (count the frequency of each bigram)\n",
    "for w1, w2 in bi_grams:\n",
    "    bigram_model[w1][w2] += 1\n",
    "\n",
    "# Convert counts to probabilities\n",
    "for w1 in bigram_model:\n",
    "    total_count = float(sum(bigram_model[w1].values()))\n",
    "    for w2 in bigram_model[w1]:\n",
    "        bigram_model[w1][w2] /= total_count\n",
    "\n",
    "# Function to predict the next word based on a given word\n",
    "def predict_next_word_bigram(bigram_model, w1):\n",
    "    next_word = bigram_model[w1]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to generate a sequence of words using bigram model\n",
    "def generate_sentence_bigram(bigram_model, w1, num_words=10):\n",
    "    sentence = [w1]\n",
    "    for _ in range(num_words):\n",
    "        next_word = predict_next_word_bigram(bigram_model, w1)\n",
    "        if not next_word:\n",
    "            break\n",
    "        sentence.append(next_word)\n",
    "        w1 = next_word  # Shift to the next word\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Test the bigram-based sentence generation\n",
    "print(generate_sentence_bigram(bigram_model, 'the', 10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the U . The company said . The company said .\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d8f2f58ee7ef38dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:15:24.675961Z",
     "start_time": "2025-02-28T21:15:19.741208Z"
    }
   },
   "source": [
    "# Create the 4-gram model\n",
    "quad_grams = list(ngrams(words, 4))\n",
    "\n",
    "# Initialize a defaultdict for the 4-gram model\n",
    "fourgram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Build the 4-gram model (count the frequency of each 4-gram)\n",
    "for w1, w2, w3, w4 in quad_grams:\n",
    "    fourgram_model[(w1, w2, w3)][w4] += 1\n",
    "\n",
    "# Convert counts to probabilities\n",
    "for w1_w2_w3 in fourgram_model:\n",
    "    total_count = float(sum(fourgram_model[w1_w2_w3].values()))\n",
    "    for w4 in fourgram_model[w1_w2_w3]:\n",
    "        fourgram_model[w1_w2_w3][w4] /= total_count\n",
    "\n",
    "# Function to predict the next word based on three previous words\n",
    "def predict_next_word_fourgram(fourgram_model, w1, w2, w3):\n",
    "    next_word = fourgram_model[w1, w2, w3]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to generate a sequence of words using the 4-gram model\n",
    "def generate_sentence_fourgram(fourgram_model, w1, w2, w3, num_words=10):\n",
    "    sentence = [w1, w2, w3]\n",
    "    for _ in range(num_words):\n",
    "        next_word = predict_next_word_fourgram(fourgram_model, w1, w2, w3)\n",
    "        if not next_word:\n",
    "            break\n",
    "        sentence.append(next_word)\n",
    "        w1, w2, w3 = w2, w3, next_word  # Shift to the next set of words for prediction\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Test the 4-gram-based sentence generation\n",
    "print(generate_sentence_fourgram(fourgram_model, 'the', 'stock', 'market', 10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stock market collapse -- prices fell nearly 15 pct -- means that\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "d786fac7a08f478d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T21:15:55.802882Z",
     "start_time": "2025-02-28T21:15:50.234582Z"
    }
   },
   "source": [
    "# Create the 5-gram model\n",
    "five_grams = list(ngrams(words, 5))\n",
    "\n",
    "# Initialize a defaultdict for the 5-gram model\n",
    "fivegram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Build the 5-gram model (count the frequency of each 5-gram)\n",
    "for w1, w2, w3, w4, w5 in five_grams:\n",
    "    fivegram_model[(w1, w2, w3, w4)][w5] += 1\n",
    "\n",
    "# Convert counts to probabilities\n",
    "for w1_w2_w3_w4 in fivegram_model:\n",
    "    total_count = float(sum(fivegram_model[w1_w2_w3_w4].values()))\n",
    "    for w5 in fivegram_model[w1_w2_w3_w4]:\n",
    "        fivegram_model[w1_w2_w3_w4][w5] /= total_count\n",
    "\n",
    "# Function to predict the next word based on four previous words\n",
    "def predict_next_word_fivegram(fivegram_model, w1, w2, w3, w4):\n",
    "    next_word = fivegram_model[w1, w2, w3, w4]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to generate a sequence of words using the 5-gram model\n",
    "def generate_sentence_fivegram(fivegram_model, w1, w2, w3, w4, num_words=10):\n",
    "    sentence = [w1, w2, w3, w4]\n",
    "    for _ in range(num_words):\n",
    "        next_word = predict_next_word_fivegram(fivegram_model, w1, w2, w3, w4)\n",
    "        if not next_word:\n",
    "            break\n",
    "        sentence.append(next_word)\n",
    "        w1, w2, w3, w4 = w2, w3, w4, next_word  # Shift to the next set of words for prediction\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Test the 5-gram-based sentence generation"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c16750517a20397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T11:22:28.829870Z",
     "start_time": "2024-10-06T11:22:28.825480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stock market collapse as companies have shifted funds away from financial investments to\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence_fivegram(fivegram_model, 'the', 'stock', 'market', 'collapse', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fcdfb5a5365ca",
   "metadata": {},
   "source": [
    "Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6342dc81a347a",
   "metadata": {},
   "source": [
    "\n",
    "    Bigram Model (n=2):\\ \"the U . The company said . The company said .\"\n",
    "\n",
    "Observation:\\ The bigram model generates short, repetitive phrases. It lacks context and often ends up repeating common phrases without much semantic coherence. Since the context window is small (only two words), the model struggles to maintain a meaningful or coherent sentence over a long sequence.\n",
    "\n",
    "    Trigram Model (n=3):\\ \"the stock of the company ' s & lt ; BP >\"\n",
    "\n",
    "Observation:\\ The trigram model improves on the bigram model by generating slightly more meaningful phrases. However, it's still prone to generating awkward or incomplete phrases (e.g., \"& lt ; BP >\"). While some local word pairs are coherent, the sentence as a whole may not make complete sense. The added context of three words allows for better prediction than bigrams, but longer sentences still suffer from inconsistency.\n",
    "\n",
    "    Fourgram Model (n=4): \"the stock market collapse -- prices fell nearly 15 pct -- means that\"\n",
    "\n",
    "Observation:\\ The fourgram model produces more coherent and contextually relevant sentences. The sentence structure is more complete, and the information conveyed makes logical sense (\"collapse -- prices fell nearly 15 pct\"). With four words of context, the model can maintain better sentence flow, though it may still end abruptly or lack sufficient grammatical closure.\n",
    "\n",
    "    Fivegram Model (n=5): \"the stock market collapse as companies have shifted funds away from financial investments to\"\n",
    "\n",
    "Observation:\\ The fivegram model produces even more coherent sentences, and the sentence begins to resemble natural language more closely. The phrase \"the stock market collapse as companies have shifted funds away from financial investments\" is well-structured and contextually meaningful. However, the sentence may still end abruptly without completing its thought (\"to\" without the next word), as the sequence generation stops after 10 words.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

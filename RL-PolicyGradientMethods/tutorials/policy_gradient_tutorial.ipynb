{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy Gradient Methods: Theory and Implementation\n",
        "\n",
        "This comprehensive tutorial covers policy gradient methods in reinforcement learning, including theoretical foundations, practical implementations, and real-world applications.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to Policy Gradient Methods](#introduction)\n",
        "2. [Theoretical Foundations](#theory)\n",
        "3. [REINFORCE Algorithm](#reinforce)\n",
        "4. [Actor-Critic Methods](#actor-critic)\n",
        "5. [Proximal Policy Optimization (PPO)](#ppo)\n",
        "6. [Real-World Case Study: Algorithmic Trading](#trading)\n",
        "7. [Comparison and Analysis](#comparison)\n",
        "8. [Advanced Topics](#advanced)\n",
        "9. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Policy Gradient Methods {#introduction}\n",
        "\n",
        "Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy function. Unlike value-based methods (like Q-learning), policy gradient methods learn the policy directly without needing to estimate value functions first.\n",
        "\n",
        "### Key Advantages:\n",
        "- Can handle continuous action spaces naturally\n",
        "- Can learn stochastic policies\n",
        "- Often more stable than value-based methods\n",
        "- Can handle high-dimensional state spaces effectively\n",
        "\n",
        "### Key Disadvantages:\n",
        "- Can have high variance in gradient estimates\n",
        "- May converge to local optima\n",
        "- Can be sample inefficient\n",
        "\n",
        "Let's start by setting up our environment and importing necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Import our custom implementations\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "from algorithms import REINFORCE, ActorCritic, PPO\n",
        "from environments import TradingEnvironment, CustomCartPoleEnv\n",
        "from utils import plot_training_progress, calculate_metrics, evaluate_policy\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Theoretical Foundations {#theory}\n",
        "\n",
        "### Policy Gradient Theorem\n",
        "\n",
        "The policy gradient theorem provides the foundation for policy gradient methods. It states that the gradient of the expected return with respect to the policy parameters can be expressed as:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\hat{A}_t \\right]$$\n",
        "\n",
        "Where:\n",
        "- $J(\\theta)$ is the expected return\n",
        "- $\\pi_\\theta(a_t|s_t)$ is the policy probability of action $a_t$ given state $s_t$\n",
        "- $\\hat{A}_t$ is an estimate of the advantage function\n",
        "- $\\tau$ is a trajectory\n",
        "\n",
        "### Advantage Function\n",
        "\n",
        "The advantage function measures how much better an action is compared to the average:\n",
        "\n",
        "$$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$$\n",
        "\n",
        "Where:\n",
        "- $Q^\\pi(s,a)$ is the action-value function\n",
        "- $V^\\pi(s)$ is the state-value function\n",
        "\n",
        "### Different Estimators\n",
        "\n",
        "1. **Monte Carlo (REINFORCE)**: $\\hat{A}_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$\n",
        "2. **Actor-Critic**: $\\hat{A}_t = r_t + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)$\n",
        "3. **GAE (Generalized Advantage Estimation)**: Combines multiple estimates\n",
        "\n",
        "Let's visualize the policy gradient concept:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize policy gradient concept\n",
        "def visualize_policy_gradient():\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Policy before and after update\n",
        "    actions = np.array([0, 1, 2])\n",
        "    \n",
        "    # Before update (random policy)\n",
        "    probs_before = np.array([0.33, 0.33, 0.34])\n",
        "    \n",
        "    # After update (biased towards action 1)\n",
        "    probs_after = np.array([0.1, 0.7, 0.2])\n",
        "    \n",
        "    ax1.bar(actions, probs_before, alpha=0.7, label='Before Update', color='lightblue')\n",
        "    ax1.bar(actions, probs_after, alpha=0.7, label='After Update', color='darkblue')\n",
        "    ax1.set_xlabel('Action')\n",
        "    ax1.set_ylabel('Probability')\n",
        "    ax1.set_title('Policy Update Example')\n",
        "    ax1.legend()\n",
        "    ax1.set_xticks(actions)\n",
        "    \n",
        "    # Advantage function visualization\n",
        "    states = np.linspace(0, 10, 100)\n",
        "    advantages = np.sin(states) * np.exp(-states/5)  # Example advantage function\n",
        "    \n",
        "    ax2.plot(states, advantages, linewidth=2, label='Advantage Function')\n",
        "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    ax2.set_xlabel('State')\n",
        "    ax2.set_ylabel('Advantage')\n",
        "    ax2.set_title('Advantage Function Example')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_policy_gradient()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
